# -*- coding: utf-8 -*-
"""Copy of Student_mentalhealth_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eRayDZsqk7bU-eaLa_PLMeQApbMQepqP
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.ensemble import RandomForestClassifier
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
from openpyxl import Workbook
from openpyxl.styles import PatternFill

# XGBoost & LightGBM
from xgboost import XGBClassifier, plot_importance
from lightgbm import LGBMClassifier, plot_importance as lgb_plot_importance

# For auto-download in Google Colab
try:
    from google.colab import files
except:
    files = None

# ==============================
# STEP 1: Load dataset
# ==============================
df = pd.read_csv("/content/students_mental_health_survey (1) (1).csv")

# Keep original copy for output
df_original = df.copy()

# ==============================
# STEP 2: Add 3000 synthetic rows
# ==============================
synthetic = df.sample(3000, replace=True).copy()
for col in df.select_dtypes(include=[np.number]).columns:
    synthetic[col] += np.random.normal(0, 0.005, size=len(synthetic))
df_aug = pd.concat([df, synthetic], ignore_index=True)
df_aug_original = pd.concat([df_original, synthetic], ignore_index=True)  # For output

# ==============================
# STEP 3: Build Mental Health Index (MHI)
# ==============================
numeric_cols = df_aug.select_dtypes(include=[np.number]).columns
MHI = pd.Series(0, index=df_aug.index, dtype=float)
for col in numeric_cols:
    mean_val = df_aug[col].mean()
    if mean_val > 5:
        MHI += (10 - df_aug[col])
    else:
        MHI += df_aug[col]
df_aug["MHI"] = MHI

# ==============================
# STEP 4: Categorize into Normal/Mild/Severe
# ==============================
quantiles = df_aug["MHI"].quantile([0.33, 0.67]).values
def categorize(mhi):
    if mhi <= quantiles[0]:
        return "Normal"
    elif mhi <= quantiles[1]:
        return "Mild"
    else:
        return "Severe"

df_aug["MentalHealthCategory"] = df_aug["MHI"].apply(categorize)

# ==============================
# STEP 5: Encode features for modeling
# ==============================
X = df_aug.drop(columns=["MentalHealthCategory", "MHI"])
y = df_aug["MentalHealthCategory"]

# Encode categorical features only for training
X_encoded = X.copy()
for col in X_encoded.select_dtypes(include=["object"]).columns:
    X_encoded[col] = LabelEncoder().fit_transform(X_encoded[col].astype(str))

label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)
class_names = label_encoder.classes_

# ==============================
# STEP 6: Train/Test Split
# ==============================
X_train, X_test, y_train, y_test = train_test_split(
    X_encoded, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
)

# Keep original test features for output
X_test_original = X.iloc[X_test.index]

# ==============================
# STEP 7: Train Models
# ==============================

# Random Forest (highest accuracy)
rf = RandomForestClassifier(
    n_estimators=1000,
    max_depth=25,
    min_samples_split=2,
    min_samples_leaf=1,
    class_weight="balanced",
    bootstrap=True,
    random_state=42,
    n_jobs=-1
)
rf.fit(X_train, y_train)
rf_pred = rf.predict(X_test)
rf_acc = accuracy_score(y_test, rf_pred)

# XGBoost
xgb = XGBClassifier(
    n_estimators=300,
    max_depth=6,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    objective="multi:softmax",
    num_class=3,
    use_label_encoder=False,
    eval_metric="mlogloss"
)
xgb.fit(X_train, y_train)
xgb_pred = xgb.predict(X_test)
xgb_acc = accuracy_score(y_test, xgb_pred)

# LightGBM
lgb = LGBMClassifier(
    n_estimators=300,
    max_depth=-1,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    objective="multiclass",
    num_class=3
)
lgb.fit(X_train, y_train)
lgb_pred = lgb.predict(X_test)
lgb_acc = accuracy_score(y_test, lgb_pred)

# ==============================
# STEP 8: Accuracy Comparison
# ==============================
print("Random Forest Accuracy:", round(rf_acc*100, 2), "%")
print("XGBoost Accuracy:", round(xgb_acc*100, 2), "%")
print("LightGBM Accuracy:", round(lgb_acc*100, 2), "%")

plt.figure(figsize=(7,5))
plt.bar(["Random Forest", "XGBoost", "LightGBM"], [rf_acc*100, xgb_acc*100, lgb_acc*100],
        color=["orange", "skyblue", "lightgreen"])
plt.ylabel("Accuracy (%)")
plt.title("Model Accuracy Comparison")
plt.ylim(85, 100)
plt.show()

# ==============================
# STEP 9: Feature Importance & Confusion Matrix
# ==============================
best_pred = rf_pred
best_model_name = "Random Forest"

plt.figure(figsize=(8,6))
importances = rf.feature_importances_
indices = np.argsort(importances)[::-1][:15]
sns.barplot(x=importances[indices], y=X.columns[indices], palette="viridis")
plt.title(f"Feature Importance ({best_model_name})")
plt.show()

cm = confusion_matrix(y_test, best_pred)
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=class_names, yticklabels=class_names)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title(f"Confusion Matrix ({best_model_name})")
plt.show()

# Save Random Forest model
joblib.dump(rf, "mental_health_model.pkl")

# ==============================
# STEP 10: Save Predictions to Excel with original textual features
# ==============================
df_results = X_test_original.copy()  # Use original text features
df_results["ActualCategory"] = label_encoder.inverse_transform(y_test)
df_results["PredictedCategory"] = label_encoder.inverse_transform(best_pred)

output_file = "mental_health_predictions.xlsx"
df_results.to_excel(output_file, index=False)

# Color-code PredictedCategory column in Excel
wb = Workbook()
ws = wb.active

# Add headers
for col_idx, col_name in enumerate(df_results.columns, 1):
    ws.cell(row=1, column=col_idx, value=col_name)

# Add data with color
for r_idx, row in enumerate(df_results.itertuples(index=False), 2):
    for c_idx, value in enumerate(row, 1):
        ws.cell(row=r_idx, column=c_idx, value=value)
        if df_results.columns[c_idx-1] == "PredictedCategory":
            if value == "Normal":
                ws.cell(row=r_idx, column=c_idx).fill = PatternFill(start_color="90EE90", fill_type="solid")
            elif value == "Mild":
                ws.cell(row=r_idx, column=c_idx).fill = PatternFill(start_color="FFFF99", fill_type="solid")
            elif value == "Severe":
                ws.cell(row=r_idx, column=c_idx).fill = PatternFill(start_color="FF7F7F", fill_type="solid")

wb.save(output_file)
print(f"Predictions saved to {output_file}")

# Auto-download in Colab
if files:
    files.download(output_file)

# ==============================
# STEP 11: Display all rows & columns in notebook with color
# ==============================
def highlight_category(val):
    if val == 'Normal':
        return 'background-color: #90EE90'
    elif val == 'Mild':
        return 'background-color: #FFFF99'
    elif val == 'Severe':
        return 'background-color: #FF7F7F'
    return ''

pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 2000)
pd.set_option('display.colheader_justify', 'center')

display(df_results.style.applymap(highlight_category, subset=['ActualCategory', 'PredictedCategory']))
